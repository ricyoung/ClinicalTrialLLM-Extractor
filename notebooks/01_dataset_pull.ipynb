{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clinical Trail Research\n",
    "\n",
    "## Clinical Trail Data Analysis - Richard Young\n",
    "\n",
    "### ryoung@unlv.edu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updated 2024-12-26\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To review / Look at for ideas\n",
    "\n",
    "https://github.com/RyanWangZf/PyTrial\n",
    "\n",
    "https://stackoverflow.com/questions/78415818/how-to-get-full-results-with-clinicaltrials-gov-api-in-python\n",
    "\n",
    "\n",
    "some code based on https://github.com/jvfe/pytrials\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import requests\n",
    "import datetime as dt\n",
    "\n",
    "import json\n",
    "import ipywidgets as widgets\n",
    "np.random.seed(10031975)\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import *\n",
    "from matplotlib.ticker import AutoMinorLocator\n",
    "import openpyxl\n",
    "import openai\n",
    "import re\n",
    "\n",
    "from urllib.parse import quote\n",
    "# import logging\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "from openai import OpenAI\n\n# Store API key in a separate file\nwith open('../config/api_key.txt', 'r') as f:\n    api_key = f.read().strip()\n\nclient = OpenAI(api_key=api_key)"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxInt = sys.maxsize\n",
    "\n",
    "while True:\n",
    "    # decrease the maxInt value by factor 10\n",
    "    # as long as the OverflowError occurs.\n",
    "\n",
    "    try:\n",
    "        csv.field_size_limit(maxInt)\n",
    "        break\n",
    "    except OverflowError:\n",
    "        maxInt = int(maxInt/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check directory for output"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "subdirs = [\n    \"../data/raw\"\n]\n\nfor subdir in subdirs:\n    os.makedirs(subdir, exist_ok=True)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note sure I need this cell anymore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all functions for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from urllib.parse import quote\n",
    "\n",
    "# Clinical Trials Calls / Functions\n",
    "\n",
    "def create_yearly_count_series(column_name):\n",
    "    yearly_counts = summary[column_name].dt.year.value_counts().sort_index()\n",
    "    # Adjusted to include the current research year\n",
    "    last_10_years = yearly_counts[(yearly_counts.index >= current_research_year - 10) & (yearly_counts.index <= current_research_year)]\n",
    "    return last_10_years.rename(column_name)\n",
    "\n",
    "# Function to save NCTid and year data for each column\n",
    "def save_nctid_year_data(column_name, file_name):\n",
    "    filtered_data = summary[summary[column_name].dt.year >= current_research_year - 10]\n",
    "    filtered_data['Year'] = filtered_data[column_name].dt.year\n",
    "    year_nctid_data = filtered_data[['Year', 'NCTid']]\n",
    "    file_path = os.path.join('data_results', file_name)\n",
    "    year_nctid_data.to_excel(file_path, index=False)\n",
    "    print(f\"{column_name} data saved at {file_path}\")\n",
    "\n",
    "def get_study_count(condition, search_area=\"condition\", page_size=1):\n",
    "    \"\"\"\n",
    "    Get the total count of studies for a given condition from ClinicalTrials.gov API.\n",
    "    \"\"\"\n",
    "    base_url = \"https://clinicaltrials.gov/api/v2/studies\"\n",
    "    encoded_condition = quote(condition)\n",
    "\n",
    "    search_area_params = {\n",
    "        \"condition\": \"query.cond\",\n",
    "        \"title\": \"query.term\",\n",
    "        \"intervention\": \"query.intr\",\n",
    "        \"outcome\": \"query.outc\",\n",
    "        \"sponsor\": \"query.spons\"\n",
    "    }\n",
    "\n",
    "    if search_area not in search_area_params:\n",
    "        print(f\"ERROR: Invalid search area: {search_area}\")\n",
    "        return None\n",
    "\n",
    "    params = {\n",
    "        search_area_params[search_area]: encoded_condition,\n",
    "        \"pageSize\": str(page_size),\n",
    "        \"countTotal\": \"true\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(base_url, params=params)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        data = response.json()\n",
    "        total_count = data.get('totalCount', 0)\n",
    "\n",
    "        print(f\"INFO: URL: {response.url}\")\n",
    "        print(f\"INFO: Total count: {total_count}\")\n",
    "\n",
    "        return total_count\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"ERROR: An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "def compare_study_counts(csv_path, api_study_count):\n",
    "    \"\"\"\n",
    "    Compare the count of unique NCTIds in the CSV file with the count from the API.\n",
    "    \"\"\"\n",
    "    # Read CSV file\n",
    "    df = pd.read_csv(csv_path)\n",
    "    unique_study_count_in_df = df['NCTId'].nunique()\n",
    "    print(f\"INFO: Unique studies in DataFrame: {unique_study_count_in_df}\")\n",
    "\n",
    "    if api_study_count is None:\n",
    "        print(\"ERROR: Failed to retrieve study count from API.\")\n",
    "        return None\n",
    "\n",
    "    print(f\"INFO: Study count from ClinicalTrials API: {api_study_count}\")\n",
    "\n",
    "    # Compare counts\n",
    "    if unique_study_count_in_df == api_study_count:\n",
    "        print(\"INFO: The counts match.\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"INFO: The counts do not match. DataFrame has {unique_study_count_in_df}, API reports {api_study_count}.\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updated for 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "disease = 'Parkinson'\n",
    "# disease =  'amyotrophic lateral sclerosis'\n",
    "# disease = 'depression'\n",
    "# disease = 'diabetes'\n",
    "# disease = 'tacs'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport pandas as pd\n\n# Define the headers\nheaders = [\n    \"Index\", \"NCTId\", \"LeadSponsorClass\", \"LeadSponsorName\", \"Condition\", \"OfficialTitle\",\n    \"BriefTitle\", \"Acronym\", \"StudyType\", \"InterventionType\", \"InterventionName\",\n    \"InterventionOtherName\", \"InterventionDescription\", \"Phase\", \"StudyFirstSubmitDate\",\n    \"LastUpdateSubmitDate\", \"CompletionDate\", \"OverallStatus\", \"BriefSummary\",\n    \"IsFDARegulatedDevice\", \"StartDate\", \"DetailedDescription\", \"ConditionMeshTerm\",\n    \"PrimaryOutcomeDescription\", \"SecondaryOutcomeDescription\", \"EnrollmentCount\",\n    \"EnrollmentType\", \"BaselineCategoryTitle\", \"BaselinePopulationDescription\",\n    \"BaselineTypeUnitsAnalyzed\", \"OtherOutcomeDescription\", \"EligibilityCriteria\",\n    \"StudyPopulation\", \"HealthyVolunteers\", \"ReferencePMID\", \"LocationCountry\",\n    \"PrimaryOutcomeTimeFrame\", \"BaselineMeasureTitle\", \"BaselineMeasureUnitOfMeasure\",\n    \"BaselineMeasurementValue\", \"GPT_summary\",\n]\n\n# Set your parameters\ncsv_file_path = os.path.join('../data/raw', f'01_{disease.lower()}_done.csv')\n\n# Get the total study count\ntotal_study_count = get_study_count(disease)\n\n# Check if the file exists\nif not os.path.exists(csv_file_path):\n    # Create the file and write the headers\n    with open(csv_file_path, 'w') as file:\n        file.write(','.join(headers) + '\\n')\n\nskip_next_cells = False\n\n# Ensure the total study count is correctly retrieved\nif total_study_count is not None:\n    # Save the API count to a DataFrame\n    api_count_df = pd.DataFrame({'Condition': [disease], 'API_Study_Count': [total_study_count]})\n    print(api_count_df.to_string(index=False))\n\n    # Compare the counts\n    counts_match = compare_study_counts(csv_file_path, total_study_count)\n\n    if counts_match is None:\n        print(\"ERROR: Comparison failed due to API error.\")\n        skip_next_cells = True  # Skip next cells because of API error\n    elif counts_match:\n        print(\"INFO: Skipping next cells as counts match.\")\n        skip_next_cells = True  # Skip next cells because counts match\n    else:\n        print(\"INFO: Proceeding with data update...\")\n        skip_next_cells = False  # Do not skip cells because counts do not match\n\ndef compare_study_counts(csv_path, api_study_count):\n    # Read CSV file\n    df = pd.read_csv(csv_path)\n    if 'NCTId' not in df.columns:\n        print(\"ERROR: NCTId column not found in the CSV file.\")\n        return None\n    unique_study_count_in_df = df['NCTId'].nunique()\n    print(f\"INFO: Unique studies in DataFrame: {unique_study_count_in_df}\")\n    print(f\"INFO: Study count from ClinicalTrials API: {api_study_count}\")\n\n    if api_study_count is None:\n        return None\n\n    if unique_study_count_in_df == api_study_count:\n        print(\"INFO: The counts match.\")\n    else:\n        print(f\"INFO: The counts do not match. DataFrame has {unique_study_count_in_df}, API reports {api_study_count}.\")\n\n    return unique_study_count_in_df == api_study_count\n\n# Example usage\nprint(f\"INFO: URL: https://clinicaltrials.gov/api/v2/studies?query.cond={disease}&pageSize=1&countTotal=true\")\nprint(f\"INFO: Total count: {total_study_count}\")"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_studies(search_terms, total_study_count, iter_size=987):\n",
    "    \"\"\"\n",
    "    Fetches study data in chunks from ClinicalTrials.gov API and processes it into a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    search_terms (dict): Dictionary with keys as search areas and values as search terms.\n",
    "    total_study_count (int): Total number of studies to fetch.\n",
    "    iter_size (int, optional): Number of studies to fetch at a time. Default is 987.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame containing the fetched study data.\n",
    "    \"\"\"\n",
    "    base_url = \"https://clinicaltrials.gov/api/v2/studies\"\n",
    "\n",
    "    search_area_params = {\n",
    "        \"condition\": \"query.cond\",\n",
    "        \"title\": \"query.term\",\n",
    "        \"intervention\": \"query.intr\",\n",
    "        \"outcome\": \"query.outc\",\n",
    "        \"sponsor\": \"query.spons\"\n",
    "    }\n",
    "\n",
    "    params = {\n",
    "        \"pageSize\": str(iter_size),\n",
    "        \"countTotal\": \"true\"\n",
    "    }\n",
    "\n",
    "    # Build search parameters based on search_terms\n",
    "    for area, term in search_terms.items():\n",
    "        if area in search_area_params:\n",
    "            params[search_area_params[area]] = term\n",
    "        else:\n",
    "            print(f\"ERROR: Invalid search area: {area}\")\n",
    "            return None\n",
    "\n",
    "    data_list = []  # List to store each chunk of data\n",
    "\n",
    "    while True:\n",
    "        # Print the current URL (for debugging purposes)\n",
    "        print(\"Fetching data from:\", base_url + '?' + '&'.join([f\"{k}={v}\" for k, v in params.items()]))\n",
    "\n",
    "        try:\n",
    "            response = requests.get(base_url, params=params)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            data = response.json()\n",
    "            studies = data.get('studies', [])\n",
    "\n",
    "            for study in studies:\n",
    "                nctId = study['protocolSection']['identificationModule'].get('nctId', 'Unknown')\n",
    "                overallStatus = study['protocolSection']['statusModule'].get('overallStatus', 'Unknown')\n",
    "                startDate = study['protocolSection']['statusModule'].get('startDateStruct', {}).get('date', 'Unknown Date')\n",
    "                conditions = ', '.join(study['protocolSection']['conditionsModule'].get('conditions', ['No conditions listed']))\n",
    "                acronym = study['protocolSection']['identificationModule'].get('acronym', 'Unknown')\n",
    "\n",
    "                interventions_list = study['protocolSection'].get('armsInterventionsModule', {}).get('interventions', [])\n",
    "                interventions = ', '.join([intervention.get('interventionName', 'No intervention name listed') for intervention in interventions_list]) if interventions_list else \"No interventions listed\"\n",
    "\n",
    "                locations_list = study['protocolSection'].get('contactsLocationsModule', {}).get('locations', [])\n",
    "                locations = ', '.join([f\"{location.get('city', 'No City')} - {location.get('country', 'No Country')}\" for location in locations_list]) if locations_list else \"No locations listed\"\n",
    "\n",
    "                primaryCompletionDate = study['protocolSection']['statusModule'].get('primaryCompletionDateStruct', {}).get('date', 'Unknown Date')\n",
    "                studyFirstPostDate = study['protocolSection']['statusModule'].get('studyFirstPostDateStruct', {}).get('date', 'Unknown Date')\n",
    "                lastUpdatePostDate = study['protocolSection']['statusModule'].get('lastUpdatePostDateStruct', {}).get('date', 'Unknown Date')\n",
    "                studyType = study['protocolSection']['designModule'].get('studyType', 'Unknown')\n",
    "                phases = ', '.join(study['protocolSection']['designModule'].get('phases', ['Not Available']))\n",
    "\n",
    "                data_list.append({\n",
    "                    \"NCTId\": nctId,\n",
    "                    \"Acronym\": acronym,\n",
    "                    \"Overall Status\": overallStatus,\n",
    "                    \"Start Date\": startDate,\n",
    "                    \"Conditions\": conditions,\n",
    "                    \"Interventions\": interventions,\n",
    "                    \"Locations\": locations,\n",
    "                    \"Primary Completion Date\": primaryCompletionDate,\n",
    "                    \"Study First Post Date\": studyFirstPostDate,\n",
    "                    \"Last Update Post Date\": lastUpdatePostDate,\n",
    "                    \"Study Type\": studyType,\n",
    "                    \"Phases\": phases\n",
    "                })\n",
    "\n",
    "            if len(data_list) >= total_study_count:\n",
    "                break\n",
    "\n",
    "            nextPageToken = data.get('nextPageToken')\n",
    "            if nextPageToken:\n",
    "                params['pageToken'] = nextPageToken\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"ERROR: An error occurred: {e}\")\n",
    "            break\n",
    "\n",
    "    # Create a DataFrame from the list of dictionaries\n",
    "    df = pd.DataFrame(data_list)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Define 'search_terms' and 'total_study_count' if not already defined\nsearch_terms = {\n    \"condition\": disease.capitalize(),\n    \"title\": disease.capitalize(),\n}\n\n\n# Get the total number of studies for the disease\ntotal_study_count = get_study_count(disease)\n\n# Fetch new data from the API using the correct function\ndf_new = fetch_studies(search_terms, total_study_count)\n\n# Load existing data\ncsv_file_path = os.path.join('../data/raw', f'01_{disease.lower()}_done.csv')\ndf_existing = pd.read_csv(csv_file_path)\n\n# Ensure column names are consistent between df_existing and df_new\nprint(\"Columns in df_existing:\", df_existing.columns.tolist())\nprint(\"Columns in df_new:\", df_new.columns.tolist())\n\n# Rename columns in df_new if necessary to match df_existing\nif 'NCT ID' in df_new.columns:\n    df_new.rename(columns={'NCT ID': 'NCTId'}, inplace=True)\n\n# Compare existing data with new data\nexisting_nct_ids = set(df_existing['NCTId'])\nnew_nct_ids = set(df_new['NCTId'])\nmissing_nct_ids = new_nct_ids - existing_nct_ids\n\n# Get the missing data\ndf_missing = df_new[df_new['NCTId'].isin(missing_nct_ids)]\n\n# Merge existing data with missing data\ndf_combined = pd.concat([df_existing, df_missing], ignore_index=True)\n\ndf_combined.drop_duplicates(subset='NCTId', inplace=True)\n\n# Save the combined data to a new CSV file\noutput_csv_path = os.path.join('../data/raw', f'01_{disease.lower()}_done.csv')\ndf_combined.to_csv(output_csv_path, index=False)\n\nprint(\"Data fetching and merging complete. Combined data saved to:\", output_csv_path)"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records in df_existing: 0\n",
      "Number of records in df_new: 3985\n",
      "Number of missing records to add: 3985\n",
      "Total records after merging: 3985\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(f\"Number of records in df_existing: {len(df_existing)}\")\n",
    "print(f\"Number of records in df_new: {len(df_new)}\")\n",
    "print(f\"Number of missing records to add: {len(df_missing)}\")\n",
    "print(f\"Total records after merging: {len(df_combined)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few rows of df_existing:\n",
      "Empty DataFrame\n",
      "Columns: [Index, NCTId, LeadSponsorClass, LeadSponsorName, Condition, OfficialTitle, BriefTitle, Acronym, StudyType, InterventionType, InterventionName, InterventionOtherName, InterventionDescription, Phase, StudyFirstSubmitDate, LastUpdateSubmitDate, CompletionDate, OverallStatus, BriefSummary, IsFDARegulatedDevice, StartDate, DetailedDescription, ConditionMeshTerm, PrimaryOutcomeDescription, SecondaryOutcomeDescription, EnrollmentCount, EnrollmentType, BaselineCategoryTitle, BaselinePopulationDescription, BaselineTypeUnitsAnalyzed, OtherOutcomeDescription, EligibilityCriteria, StudyPopulation, HealthyVolunteers, ReferencePMID, LocationCountry, PrimaryOutcomeTimeFrame, BaselineMeasureTitle, BaselineMeasureUnitOfMeasure, BaselineMeasurementValue, GPT_summary]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 41 columns]\n",
      "First few rows of df_new:\n",
      "         NCTId  Acronym           Overall Status  Start Date  \\\n",
      "0  NCT01329926  Unknown                WITHDRAWN  2011-06-30   \n",
      "1  NCT02320266  Unknown  ENROLLING_BY_INVITATION     2014-12   \n",
      "2  NCT03599726  Unknown                COMPLETED  2018-07-30   \n",
      "3  NCT04148326  Unknown                  UNKNOWN  2020-03-02   \n",
      "4  NCT06147726  Unknown               RECRUITING  2023-11-01   \n",
      "\n",
      "                                          Conditions  \\\n",
      "0                  Parkinson's Disease, Parkinsonism   \n",
      "1  Parkinson's Disease, Essential Tremor, Dystoni...   \n",
      "2                                  Parkinson Disease   \n",
      "3  Parkinson Disease, Parkinsons Disease With Dem...   \n",
      "4  Parkinson Disease, Virtual Reality, Neurologic...   \n",
      "\n",
      "                                       Interventions  \\\n",
      "0                        No intervention name listed   \n",
      "1                            No interventions listed   \n",
      "2  No intervention name listed, No intervention n...   \n",
      "3                        No intervention name listed   \n",
      "4  No intervention name listed, No intervention n...   \n",
      "\n",
      "                     Locations Primary Completion Date Study First Post Date  \\\n",
      "0  Los Angeles - United States              2022-12-31            2011-04-06   \n",
      "1       Aurora - United States                 2024-12            2014-12-19   \n",
      "2     Portland - United States              2019-05-30            2018-07-26   \n",
      "3      Ventura - United States                 2023-03            2019-11-01   \n",
      "4                Bolu - Turkey              2024-11-01            2023-11-28   \n",
      "\n",
      "  Last Update Post Date      Study Type         Phases  \n",
      "0            2022-04-12   OBSERVATIONAL  Not Available  \n",
      "1            2024-05-13   OBSERVATIONAL  Not Available  \n",
      "2            2020-05-11  INTERVENTIONAL   EARLY_PHASE1  \n",
      "3            2021-09-05   OBSERVATIONAL  Not Available  \n",
      "4            2024-04-19  INTERVENTIONAL             NA  \n"
     ]
    }
   ],
   "source": [
    "print(\"First few rows of df_existing:\")\n",
    "print(df_existing.head())\n",
    "\n",
    "print(\"First few rows of df_new:\")\n",
    "print(df_new.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets Start Getting the Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_dict(d, parent_key='', sep='_', prefixes_to_replace=None):\n",
    "    \"\"\"\n",
    "    Flatten a nested dictionary, optionally replacing specified prefixes in keys.\n",
    "    \"\"\"\n",
    "    items = []\n",
    "    if prefixes_to_replace is None:\n",
    "        prefixes_to_replace = {}\n",
    "\n",
    "    if isinstance(d, dict):\n",
    "        for k, v in d.items():\n",
    "            new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n",
    "\n",
    "            # Replace specified prefixes\n",
    "            for old_prefix, new_prefix in prefixes_to_replace.items():\n",
    "                if new_key.startswith(old_prefix):\n",
    "                    new_key = new_prefix + new_key[len(old_prefix):]\n",
    "\n",
    "            if isinstance(v, dict):\n",
    "                items.extend(flatten_dict(v, new_key, sep=sep, prefixes_to_replace=prefixes_to_replace).items())\n",
    "            elif isinstance(v, list):\n",
    "                if v and all(isinstance(i, dict) for i in v):\n",
    "                    for idx, item in enumerate(v):\n",
    "                        items.extend(flatten_dict(item, f\"{new_key}{sep}{idx}\", sep=sep, prefixes_to_replace=prefixes_to_replace).items())\n",
    "                else:\n",
    "                    items.append((new_key, v))\n",
    "            else:\n",
    "                items.append((new_key, v))\n",
    "    else:\n",
    "        items.append((parent_key, d))\n",
    "    return dict(items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_study_details_batch(nct_ids, batch_size=50):\n",
    "    import requests\n",
    "    import pandas as pd\n",
    "\n",
    "    base_url = \"https://clinicaltrials.gov/api/v2/studies\"\n",
    "    data_list = []\n",
    "\n",
    "    for i in range(0, len(nct_ids), batch_size):\n",
    "        batch_nct_ids = nct_ids[i:i + batch_size]\n",
    "        nct_ids_str = ','.join(batch_nct_ids)\n",
    "        params = {\n",
    "            \"format\": \"json\",\n",
    "            \"pageSize\": len(batch_nct_ids)\n",
    "        }\n",
    "        from urllib.parse import urlencode\n",
    "        query_string = urlencode(params)\n",
    "        request_url = f\"{base_url}?filter.ids={nct_ids_str}&{query_string}\"\n",
    "        print(f\"Fetching data from: {request_url}\")\n",
    "\n",
    "        response = requests.get(request_url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            studies = data.get('studies', [])\n",
    "\n",
    "            if not studies:\n",
    "                print(f\"No studies found for NCTIds: {batch_nct_ids}\")\n",
    "                continue\n",
    "\n",
    "            for study in studies:\n",
    "                # Extract relevant sections\n",
    "                protocol = study.get('protocolSection', {})\n",
    "                identificationModule = protocol.get('identificationModule', {})\n",
    "                statusModule = protocol.get('statusModule', {})\n",
    "                sponsorModule = protocol.get('sponsorCollaboratorsModule', {})\n",
    "                descriptionModule = protocol.get('descriptionModule', {})\n",
    "                conditionsModule = protocol.get('conditionsModule', {})\n",
    "                designModule = protocol.get('designModule', {})\n",
    "                armsInterventionsModule = protocol.get('armsInterventionsModule', {})\n",
    "                outcomesModule = protocol.get('outcomesModule', {})\n",
    "                eligibilityModule = protocol.get('eligibilityModule', {})\n",
    "                contactsModule = protocol.get('contactsLocationsModule', {})\n",
    "                referencesModule = protocol.get('referencesModule', {})\n",
    "\n",
    "                # Process LocationCountry field with error handling\n",
    "                locations_data = contactsModule.get('locations', [])\n",
    "                location_countries = []\n",
    "\n",
    "                try:\n",
    "                    if isinstance(locations_data, list):\n",
    "                        for loc in locations_data:\n",
    "                            if isinstance(loc, dict):\n",
    "                                facility = loc.get('facility', {})\n",
    "                                if isinstance(facility, dict):\n",
    "                                    country = facility.get('country', '')\n",
    "                                    if country:\n",
    "                                        location_countries.append(country)\n",
    "                    elif isinstance(locations_data, dict):\n",
    "                        facility = locations_data.get('facility', {})\n",
    "                        if isinstance(facility, dict):\n",
    "                            country = facility.get('country', '')\n",
    "                            if country:\n",
    "                                location_countries.append(country)\n",
    "                    elif isinstance(locations_data, str):\n",
    "                        location_countries.append(locations_data)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing locations for study {identificationModule.get('nctId', '')}: {e}\")\n",
    "                    location_countries = []\n",
    "\n",
    "                # Extract specific fields\n",
    "                study_data = {\n",
    "                    'NCTId': identificationModule.get('nctId', ''),\n",
    "                    'LeadSponsorClass': sponsorModule.get('leadSponsor', {}).get('class', ''),\n",
    "                    'LeadSponsorName': sponsorModule.get('leadSponsor', {}).get('name', ''),\n",
    "                    'Condition': ', '.join(conditionsModule.get('conditions', [])),\n",
    "                    'OfficialTitle': identificationModule.get('officialTitle', ''),\n",
    "                    'BriefTitle': identificationModule.get('briefTitle', ''),\n",
    "                    'Acronym': identificationModule.get('acronym', ''),\n",
    "                    'StudyType': designModule.get('studyType', ''),\n",
    "                    'InterventionType': ', '.join([intervention.get('interventionType', '') for intervention in armsInterventionsModule.get('interventions', [])]),\n",
    "                    'InterventionName': ', '.join([intervention.get('interventionName', intervention.get('name', '')) for intervention in armsInterventionsModule.get('interventions', [])]),\n",
    "                    'InterventionOtherName': ', '.join([', '.join(intervention.get('otherName', [])) for intervention in armsInterventionsModule.get('interventions', [])]),\n",
    "                    'InterventionDescription': ', '.join([intervention.get('description', '') for intervention in armsInterventionsModule.get('interventions', [])]),\n",
    "                    'Phase': ', '.join(designModule.get('phases', [])),\n",
    "                    'StudyFirstSubmitDate': statusModule.get('studyFirstSubmitDate', ''),\n",
    "                    'LastUpdateSubmitDate': statusModule.get('lastUpdateSubmitDate', ''),\n",
    "                    'CompletionDate': statusModule.get('completionDateStruct', {}).get('date', ''),\n",
    "                    'OverallStatus': statusModule.get('overallStatus', ''),\n",
    "                    'BriefSummary': descriptionModule.get('briefSummary', ''),\n",
    "                    'IsFDARegulatedDevice': protocol.get('oversightModule', {}).get('isFdaRegulatedDevice', ''),\n",
    "                    'StartDate': statusModule.get('startDateStruct', {}).get('date', ''),\n",
    "                    'DetailedDescription': descriptionModule.get('detailedDescription', ''),\n",
    "                    'ConditionMeshTerm': ', '.join([mesh.get('term', '') for mesh in protocol.get('derivedSection', {}).get('conditionBrowseModule', {}).get('meshes', [])]),\n",
    "                    'PrimaryOutcomeDescription': ', '.join([outcome.get('description', '') for outcome in outcomesModule.get('primaryOutcomes', [])]),\n",
    "                    'SecondaryOutcomeDescription': ', '.join([outcome.get('description', '') for outcome in outcomesModule.get('secondaryOutcomes', [])]),\n",
    "                    'EnrollmentCount': designModule.get('enrollmentInfo', {}).get('count', ''),\n",
    "                    'EnrollmentType': designModule.get('enrollmentInfo', {}).get('type', ''),\n",
    "                    'EligibilityCriteria': eligibilityModule.get('eligibilityCriteria', ''),\n",
    "                    'StudyPopulation': eligibilityModule.get('studyPopulation', ''),\n",
    "                    'HealthyVolunteers': eligibilityModule.get('healthyVolunteers', ''),\n",
    "                    'ReferencePMID': ', '.join([ref.get('pmid', '') for ref in referencesModule.get('references', []) if 'pmid' in ref]),\n",
    "                    'LocationCountry': ', '.join(location_countries),\n",
    "                    'PrimaryOutcomeTimeFrame': ', '.join([outcome.get('timeFrame', '') for outcome in outcomesModule.get('primaryOutcomes', [])]),\n",
    "                    # Add other fields as required\n",
    "                }\n",
    "\n",
    "                # Append to data list\n",
    "                data_list.append(study_data)\n",
    "\n",
    "        else:\n",
    "            print(f\"Failed to fetch data. Status code: {response.status_code}\")\n",
    "            print(f\"Response content: {response.text}\")\n",
    "            continue\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data_list)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Compare existing data with new data\nexisting_nct_ids = set(df_existing['NCTId'])\nnew_nct_ids = set(df_new['NCTId'])\nmissing_nct_ids = list(new_nct_ids - existing_nct_ids)\n\nprint(f\"Number of missing NCTIds to fetch: {len(missing_nct_ids)}\")\n\n# Fetch missing data using the updated function\ndf_missing_details = fetch_study_details_batch(missing_nct_ids, batch_size=25)\n\n# Verify the data fetched\nprint(\"First few rows of df_missing_details:\")\nprint(df_missing_details.head())\n\n# Check if df_missing_details is empty\nif df_missing_details.empty:\n    print(\"No data was fetched for the missing NCTIds.\")\nelse:\n    # Combine with existing data\n    df_combined = pd.concat([df_existing, df_missing_details], ignore_index=True)\n    df_combined.drop_duplicates(subset='NCTId', inplace=True)\n\n    # Save the combined data\n    output_csv_path_csv = os.path.join('../data/raw', f'01_{disease.lower()}_done.csv')\n    output_excel_path = os.path.join('../data/raw', f'01_{disease.lower()}_done.xlsx')\n\n    df_combined.to_csv(output_csv_path_csv, index=False)\n    df_combined.to_excel(output_excel_path, index=False)\n\n    print(\"Data fetching and merging complete. Combined data saved to:\", output_csv_path_csv)"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Index        NCTId LeadSponsorClass  \\\n",
      "0   NaN  NCT04114994         INDUSTRY   \n",
      "1   NaN  NCT00292227         INDUSTRY   \n",
      "2   NaN  NCT02138682         INDUSTRY   \n",
      "3   NaN  NCT05034094            OTHER   \n",
      "4   NaN  NCT05292794         INDUSTRY   \n",
      "\n",
      "                                     LeadSponsorName  \\\n",
      "0                              Alzheimer's Light LLC   \n",
      "1                                         UCB Pharma   \n",
      "2  Wisconsin Institute for Neurologic and Sleep D...   \n",
      "3                                  Assiut University   \n",
      "4                                      CereGate Inc.   \n",
      "\n",
      "                                           Condition  \\\n",
      "0  Alzheimer Disease, Mild Cognitive Impairment, ...   \n",
      "1                                Parkinson's Disease   \n",
      "2              Parkinson Disease, Movement Disorders   \n",
      "3                                  Parkinson Disease   \n",
      "4  Parkinson Disease, Freezing of Gait, Deep Brai...   \n",
      "\n",
      "                                       OfficialTitle  \\\n",
      "0          Longitudinal Cognitive Assessment by BoCA   \n",
      "1  Double-blind, Randomized, Placebo- and Positiv...   \n",
      "2  Validation of DaTscan for Detection of Parkins...   \n",
      "3  Auditory and Vestibular Functions in Patients ...   \n",
      "4  A Multi-Center, Controlled Study to Evaluate U...   \n",
      "\n",
      "                                          BriefTitle Acronym       StudyType  \\\n",
      "0          Longitudinal Cognitive Assessment by BoCA    BoCA   OBSERVATIONAL   \n",
      "1  Cardiac Effects of Rotigotine Transdermal Syst...          INTERVENTIONAL   \n",
      "2  Validation of DaTscan for Detection of Parkins...           OBSERVATIONAL   \n",
      "3  Auditory and Vestibular Function in Parkinson'...           OBSERVATIONAL   \n",
      "4  Use of CereGate Therapy for Freezing of Gait i...          INTERVENTIONAL   \n",
      "\n",
      "  InterventionType  ...                                EligibilityCriteria  \\\n",
      "0                   ...  Inclusion Criteria:\\n\\nage 50 or older\\n\\nExcl...   \n",
      "1         , , , ,   ...  Inclusion Criteria:\\n\\n* Male or female at lea...   \n",
      "2                   ...  Inclusion Criteria:\\n\\n* aged 75 or older\\n* s...   \n",
      "3                   ...  Inclusion Criteria:\\n\\n* Adults with age range...   \n",
      "4                   ...  Inclusion Criteria:\\n\\n1. Participant has an i...   \n",
      "\n",
      "                                     StudyPopulation HealthyVolunteers  \\\n",
      "0  According to the American Academy of Neurology...              True   \n",
      "1                                                                False   \n",
      "2  Parkinson disease patients from Wisconsin Inst...             False   \n",
      "3  * Patients with PD will be recruited from the ...                     \n",
      "4                                                                False   \n",
      "\n",
      "        ReferencePMID LocationCountry  \\\n",
      "0            31534026                   \n",
      "1  18650802, 22401642                   \n",
      "2                                       \n",
      "3  30619045, 32032926                   \n",
      "4                                       \n",
      "\n",
      "                             PrimaryOutcomeTimeFrame BaselineMeasureTitle  \\\n",
      "0     through study completion, an average of 1 year                  NaN   \n",
      "1  Baseline (Day -2/ Day -1) 20:00h, Day 42 20:00...                  NaN   \n",
      "2  Will be assessed upon receipt of autopsy repor...                  NaN   \n",
      "3                                           16 month                  NaN   \n",
      "4  Pre-CG therapy to post-CG-therapy follow-up Da...                  NaN   \n",
      "\n",
      "  BaselineMeasureUnitOfMeasure BaselineMeasurementValue GPT_summary  \n",
      "0                          NaN                      NaN         NaN  \n",
      "1                          NaN                      NaN         NaN  \n",
      "2                          NaN                      NaN         NaN  \n",
      "3                          NaN                      NaN         NaN  \n",
      "4                          NaN                      NaN         NaN  \n",
      "\n",
      "[5 rows x 41 columns]\n"
     ]
    }
   ],
   "source": [
    "# Check if 'GPT_summary' column exists, if not, add it\n",
    "if 'GPT_summary' not in df_combined.columns:\n",
    "    df_combined.insert(3, 'GPT_summary', '')  # Insert as the 4th column (index 3)\n",
    "\n",
    "print(df_combined.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = df_combined.copy()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport pandas as pd\nimport numpy as np\n\n# Assuming df_combined is your DataFrame\ndf_combined['IsFDARegulatedDevice'] = df_combined['IsFDARegulatedDevice'].replace('', np.nan)\n\n# Ensure 'IsFDARegulatedDevice' column contains only strings or NaN\ndf_combined['IsFDARegulatedDevice'] = df_combined['IsFDARegulatedDevice'].astype(str).replace('nan', np.nan)\n\n# Replace empty strings in 'HealthyVolunteers' with NaN\ndf_combined['HealthyVolunteers'] = df_combined['HealthyVolunteers'].replace('', np.nan)\n\n# Ensure 'HealthyVolunteers' column contains only strings or NaN\ndf_combined['HealthyVolunteers'] = df_combined['HealthyVolunteers'].astype(str).replace('nan', np.nan)\n\n# Convert 'EnrollmentCount' to numeric, coercing errors to NaN\ndf_combined['EnrollmentCount'] = pd.to_numeric(df_combined['EnrollmentCount'], errors='coerce')\n\n# Save to CSV\ncsv_file_path = os.path.join('../data/raw', f'01_{disease.lower()}_done.csv')\ndf_combined.to_csv(csv_file_path, index=False)\n\n# Save to Parquet\nparquet_file_path = os.path.join('../data/raw', f'01_{disease.lower()}.parquet')\ndf_combined.to_parquet(parquet_file_path, index=False)\n\nprint(f\"Data saved to {csv_file_path} and {parquet_file_path}\")"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'success': True, 'textId': '4861735233914382', 'quotaRemaining': 597}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "\n",
    "resp = requests.post('https://textbelt.com/text', {\n",
    "\n",
    "  'phone': '9163802941',\n",
    "\n",
    "  'message': 'PD Pipe Line Part 1 Done',\n",
    "\n",
    "  'key': '138adc496234ca311154757db147f552afa8ba83FfrCKJ36kTJNXq65nlsvvF4Pu',\n",
    "\n",
    "})\n",
    "\n",
    "print(resp.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Infection point - both new and old pulls should be the same\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets Do some data cleaning "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PD_research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}